# MPI DISTRIBUTED TRAINING

Implementation of Distributed training methods like:
1. DATA PARALLELISM
2. MODEL PARALLELISM
3. GPIPE 

## Libraries Used 
-  MPI4PY - for distributed communication
-  Pytorch - for Neural Network Training 
-  Torchvision - for datasets 

## USAGE
1. Make sure to install the requirements.txt file using 
   ```
   `pip install -r requirements.txt`   
2. Run which ever method you want to try using 
    `mpirun -np 4 python3 data_para.py`

### TODO 
- PIPEDREAM method Implementation
```

